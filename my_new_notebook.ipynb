{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not find kaggle.json. Make sure it's located in /Users/tonderaimadamba/.kaggle. Or use the environment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DistilBertModel\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata_preprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m injestDataset, downSample\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata_cleaning\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_cleaner, cleaningPreprocess\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m~/nlp-classification/data_preprocessing.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m resample\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkaggle\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m sample\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msubprocess\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/kaggle/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkaggle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_client\u001b[39;00m \u001b[39mimport\u001b[39;00m ApiClient\n\u001b[1;32m     22\u001b[0m api \u001b[39m=\u001b[39m KaggleApi(ApiClient())\n\u001b[0;32m---> 23\u001b[0m api\u001b[39m.\u001b[39;49mauthenticate()\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/kaggle/api/kaggle_api_extended.py:403\u001b[0m, in \u001b[0;36mKaggleApi.authenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         config_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_config_file(config_data)\n\u001b[1;32m    402\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. Make sure it\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms located in\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    404\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. Or use the environment method.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    405\u001b[0m                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_file, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_dir))\n\u001b[1;32m    407\u001b[0m \u001b[39m# Step 3: load into configuration!\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_config(config_data)\n",
      "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /Users/tonderaimadamba/.kaggle. Or use the environment method."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR  #\n",
    "# from torch.utils.data import TensorDataset, \\\n",
    "#     DataLoader  # Own stuff set of data quality checks, tensor shapes might be different dataloader loads the tensor,\n",
    "import pytorch_lightning as pl  #\n",
    "from pytorch_lightning.callbacks.early_stopping import \\\n",
    "    EarlyStopping  # early stop when you reach optimum loss, 3 times in a row gradient descent\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor  # delta (loss / accuracy)\n",
    "from pytorch_lightning.loggers import MLFlowLogger  # Model tracking\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "from data_preprocessing import injestDataset, downSample\n",
    "from data_cleaning import feature_cleaner, cleaningPreprocess\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, LabelBinarizer\n",
    "\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import numpy as np\n",
    "# from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injest data see injestDataset and down sampling to make levels/classes the same size \n",
    "* the injested file is kaggle data https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews\n",
    "* the goal is to predict rating using the review or recommended(1 positive 0 negative)\n",
    "* would also like to explore multimodal aproach given recomendation can I predict rating or recommend \n",
    "* some sort of UI gladio \n",
    "* MLOps serialize weights or save weights and broadcast need to explore that \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Womens Clothing E-Commerce Reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tonderaimadamba/nlp-classification/my_new_notebook.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m injestDataset()\n",
      "File \u001b[0;32m~/nlp-classification/data_preprocessing.py:11\u001b[0m, in \u001b[0;36minjestDataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minjestDataset\u001b[39m():\n\u001b[1;32m     10\u001b[0m     load_dir \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39mdata/Womens Clothing E-Commerce Reviews.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(load_dir, index_col\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, header\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     df \u001b[39m=\u001b[39m data[[\u001b[39m'\u001b[39m\u001b[39mReview Text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRecommended IND\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     13\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdropna(how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39many\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/nlp-classification/pytorch_env/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Womens Clothing E-Commerce Reviews.csv'"
     ]
    }
   ],
   "source": [
    "data = injestDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df = downSample(data,\"Recommended IND\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying NLTK to clean data check data_cleaning.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df = cleaningPreprocess(df, \"Review Text\") #  Apply cleaner \n",
    "df = df.rename(columns={\"Recommended IND\": \"targetOne\", \"Review Text\": \"feature\", \"Rating\": \"targetTwo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dataset class\n",
    "* Really like how spark does the data transformations and adds column after every transformation in a pipeline attempiting to do the same \n",
    "* We going to have two lines one for the normal __getitem__ meaning dataloader should be able to get the tensors for input and output plus also an added bonus to view transformations in dataframe format \n",
    "* try something different use self to execute \n",
    "* use decorators and also self to activate or deactivate the instance of viewing the dataframe or tensors \n",
    "* also fix __len__ for when train is on (inside decorator) it should show the train length or test length etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Here is your dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_data = df\n",
    "        # self.max_length = 30\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.tokenize_dataframe()\n",
    "        self.train_val_test_idx()\n",
    "        self.data_split = None\n",
    "        self.split_columns()\n",
    "        self.targertOne_labelEncoder()\n",
    "        self.targertTwo_labelEncoder()\n",
    "\n",
    "    def tokenize_dataframe(self):\n",
    "        tokenized_texts = []\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for text in self.input_data['feature']:\n",
    "            encoded_inputs = self.tokenizer(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                # max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            tokenized_texts.append(encoded_inputs)\n",
    "            input_ids.append(encoded_inputs['input_ids'])\n",
    "            attention_masks.append(encoded_inputs['attention_mask'])\n",
    "        \n",
    "        self.input_data['tokenizedFeature'] = tokenized_texts\n",
    "        self.input_data['input_ids'] = input_ids\n",
    "        self.input_data['attention_mask'] = attention_masks\n",
    "\n",
    "\n",
    "    def split_columns(self):\n",
    "        self.features = self.input_data[\"feature\"]\n",
    "        self.targetOne = self.input_data[\"targetOne\"]\n",
    "        self.targetTwo = self.input_data[\"targetTwo\"]\n",
    "\n",
    "            \n",
    "    def train_val_test_idx(self):\n",
    "        idx = list(range(len(self.input_data)))\n",
    "        train_idx, val_test_idx = train_test_split(\n",
    "                                idx,\n",
    "                                train_size=0.8,\n",
    "                                stratify=self.input_data[\"targetOne\"],\n",
    "                                random_state=500)\n",
    "                                \n",
    "        test_idx,  val_idx, = train_test_split(\n",
    "                                val_test_idx,\n",
    "                                train_size=0.5,\n",
    "                                random_state=500)\n",
    "\n",
    "        self.train_idx = train_idx \n",
    "        self.test_idx = test_idx\n",
    "        self.val_idx = val_idx\n",
    "    \n",
    "    def targertOne_labelEncoder(self):\n",
    "        # self.targetOne_label_binarizer = MultiLabelBinarizer()\n",
    "        self.targetOne_label_binarizer = LabelEncoder()\n",
    "        transformed =  self.targetOne_label_binarizer.fit_transform(self.input_data[\"targetOne\"].astype(\"str\"))\n",
    "        self.input_data['targetOne_binirized'] = [subarray[0] for subarray in np.split(transformed,len(transformed))]\n",
    "\n",
    "    def targertTwo_labelEncoder(self):\n",
    "        self.targetTwo_label_binarizer = MultiLabelBinarizer()\n",
    "        transformed =  self.targetTwo_label_binarizer.fit_transform(self.input_data[\"targetTwo\"].astype(\"str\"))\n",
    "        self.input_data['targetTwo_binirized'] = [subarray[0] for subarray in np.split(transformed,len(transformed))]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.data_split is None or self.data_split == 'all':\n",
    "            # Return the length of the entire dataset\n",
    "            return len(self.input_data)\n",
    "        elif self.data_split == 'train':\n",
    "            # Return the length of the training split\n",
    "            return len(self.train_idx)\n",
    "        elif self.data_split == 'val':\n",
    "            # Return the length of the validation split\n",
    "            return len(self.val_idx)\n",
    "        elif self.data_split == 'test':\n",
    "            # Return the length of the test split\n",
    "            return len(self.test_idx)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data_split argument. Use 'train', 'val', 'test', or 'all'.\")\n",
    "        \n",
    "    ## The whole purpose of this __getitem__ function is to return a dataframe or dataloader compatible data \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.data_split is not None:\n",
    "            # Depending on the data_split argument, return the corresponding split\n",
    "            if self.data_split == 'train':\n",
    "                index = self.train_idx[index]\n",
    "            elif self.data_split == 'val':\n",
    "                index = self.val_idx[index]\n",
    "            elif self.data_split == 'test':\n",
    "                index = self.test_idx[index]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid data_split argument. Use 'train', 'val', or 'test'.\")\n",
    "\n",
    "            input_ids = self.input_data['input_ids'][index]\n",
    "            attention_mask = self.input_data['attention_mask'][index]\n",
    "            targetOne = torch.tensor(self.input_data['targetOne_binirized'][index],dtype=torch.float32,device='mps:0')\n",
    "            targetTwo = torch.tensor(self.input_data['targetTwo_binirized'][index],dtype=torch.float32,device='mps:0')\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'targetOne': targetOne,\n",
    "                'targetTwo': targetTwo,\n",
    "            }\n",
    "\n",
    "        else: \n",
    "            return self.input_data.iloc[index]\n",
    "\n",
    "       # Define properties to access different data splits\n",
    "\n",
    "\n",
    "    # def set_data_split(self, data_split):\n",
    "    #     # This method allows you to set the data_split after creating the instance\n",
    "    #     self.data_split = data_split\n",
    "\n",
    "    # def reset_data_split(self):\n",
    "    #     # This method allows you to reset the data_split to None, returning the entire DataFrame\n",
    "    #     self.data_split = None\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        self.data_split = 'train'\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        self.data_split = 'val'\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        self.data_split = 'test'\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def all(self):\n",
    "        self.data_split = None\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloader and batch size, and test the decorators if they are working \n",
    "* check data is loaded properly with the decorators and the loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "my_dataset = MyDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "my_dataset.all[1:10] # fuction .all is meant to show intire data set with all the transformations packaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "my_dataset.all[my_dataset.train_idx].iloc[1:5] # test set check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "list(my_dataset.all[my_dataset.train_idx].iloc[[1]][\"input_ids\"]) #compared to my_dataset.train[1] should be the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "my_dataset.train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "my_dataset.test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "my_dataset.val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataloader previous task and batch \n",
    "* if batch size = 5 and length = 800 shoud be 160 batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "train_loader = DataLoader(my_dataset.train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader, 0):\n",
    "    input_ids = batch['input_ids'] # Access input IDs from the batch\n",
    "    attention_mask = batch['attention_mask']  # Access attention masks from the batch\n",
    "    targetOne = batch['targetOne']  # Access targetOne from the batch\n",
    "    targetTwo = batch['targetTwo'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "i # number of batches should be = len(train set) = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "len(my_dataset.train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "targetTwo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch first\n",
    "* Hardware issue might have to migrate to databricks but need some GPUs or Colab \n",
    "* Setup neural net with binary cross entropy loss or categorical depending on labels \n",
    "* make sure to send model to device you can't use self I think but investigate you have to make the model first and then transfer it that's the assumption during self we still inside the model so shouldn't work my guess \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "input_dim = 521\n",
    "# number of hidden layers\n",
    "hidden_layers = 758\n",
    "# number of classes (unique of y)\n",
    "output_dim = 2\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.device = \"mps:0\"\n",
    "    self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    # self.pre_classifier = nn.Linear(521, 521)\n",
    "    # self.classifier = nn.Linear(521, output_dim)\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.activation = nn.ReLU()\n",
    "  def forward(self, batch):\n",
    "    embeddings = self.model(batch['input_ids'].squeeze(1).to(self.device), attention_mask=batch['attention_mask'].to(self.device), output_hidden_states= True, return_dict=False)[0]\n",
    "    pooler = self.activation(self.linear1(embeddings))\n",
    "    pooler = self.dropout(pooler)\n",
    "    output = self.linear2(pooler)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "clf = Network()\n",
    "device = torch.device(\"mps:0\")\n",
    "clf.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(clf.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['targetOne']  # Assuming 'targetOne' is your target variable\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = clf({'input_ids': inputs, 'attention_mask': attention_mask})\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Display statistics\n",
    "    print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / len(train_loader):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tonderaimadamba/nlp-classification/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#GPU too small "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
